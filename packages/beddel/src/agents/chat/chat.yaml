# Chat Agent - Native Builtin Orchestrator Agent for Beddel Runtime
# Route: /agents/chat
# Method: chat.execute

agent:
  id: chat
  version: 1.0.0
  protocol: beddel-declarative-protocol/v2.0

metadata:
  name: "Q&A Context Chat Agent"
  description: "Orchestrates RAG pipeline or simple chat based on mode selection"
  category: "chat"
  route: "/agents/chat"
  knowledge_sources:
    - "gitmcp-agent"
  tags:
    - "chat"
    - "orchestrator"
    - "rag"
    - "qa"
    - "simple"

schema:
  input:
    type: "object"
    properties:
      messages:
        type: "array"
        items:
          type: "object"
          properties:
            role:
              type: "string"
              enum: ["user", "assistant", "system"]
            content:
              type: "string"
          required: ["role", "content"]
        description: "Conversation messages"
      query:
        type: "string"
        description: "Direct query (alternative to messages)"
      mode:
        type: "string"
        enum: ["rag", "simple"]
        default: "rag"
        description: "Chat mode: 'rag' for knowledge-based answers, 'simple' for direct LLM chat"
      knowledge_sources:
        type: "array"
        items:
          type: "string"
        description: "GitMCP URLs to use as knowledge sources"
    required: ["messages"]

  output:
    type: "object"
    properties:
      response:
        type: "string"
        description: "Generated answer"
      timestamp:
        type: "string"
        description: "ISO timestamp"
      execution_steps:
        type: "array"
        description: "Detailed execution steps for observability"
      total_duration:
        type: "number"
        description: "Total execution time in milliseconds"
    required: ["response", "timestamp"]

logic:
  variables:
    - name: "userQuery"
      type: "string"
      init: '""'
    - name: "allDocuments"
      type: "string"
      init: '""'

  workflow:
    # Step 1: Extract user query from messages
    - name: "extract-query"
      type: "output-generator"
      action:
        type: "generate"
        output:
          _extracted: true

    # Step 2: Mode branch - Simple chat or RAG pipeline
    - name: "mode-check"
      type: "conditional"
      condition: "$input.mode == 'simple'"
      then:
        # Simple mode: Direct LLM chat with conversation history
        - name: "simple-chat"
          type: "rag"
          action:
            query: "$input.query"
            history: "$input.messages"
            mode: "simple"
            result: "ragResult"
      else:
        # RAG mode: Full pipeline with knowledge base
        - name: "vectorize-query"
          type: "gemini-vectorize"
          action:
            action: "embedSingle"
            text: "$input.query"
            result: "queryVector"

        - name: "check-knowledge"
          type: "chromadb"
          action:
            action: "hasData"
            collection_name: "beddel_knowledge"
            min_count: 5
            result: "hasDataResult"

        - name: "search-knowledge"
          type: "chromadb"
          action:
            action: "search"
            collection_name: "beddel_knowledge"
            query_vector: "$queryVector.vector"
            limit: 5
            result: "searchResult"

        - name: "generate-answer"
          type: "rag"
          action:
            query: "$input.query"
            documents: "$searchResult.documents"
            history: "$input.messages"
            mode: "rag"
            result: "ragResult"

    # Step 3: Deliver final response
    - name: "deliver-response"
      type: "output-generator"
      action:
        type: "generate"
        output:
          response: "$ragResult.response"
          timestamp: "$ragResult.timestamp"

output:
  schema:
    response: "$ragResult.response"
    timestamp: "$ragResult.timestamp"
